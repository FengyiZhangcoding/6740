import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.neighbors import KernelDensity
import csv
import seaborn as sb
import numpy.matlib
import pandas as pd
import scipy.sparse.linalg as ll
from mpl_toolkits.mplot3d import Axes3D
import pandas as pd

density_param = {'density': True}
path = 'n90pol.csv'
data=pd.read_csv(path,header=None).to_numpy()

amygdala=list(map(float,data[1:,0]))
acc=list(map(float,data[1:,1]))
orientation=list(map(float,data[1:,2]))
np.random.seed(1)
size=len(amygdala)

nbin = 10     # you can change the number of bins in each dimension
fig, ax = plt.subplots(2, 2)
fig.subplots_adjust(hspace=0.2, wspace=0.2)

# histogram amygdala
X=amygdala
min_data = min(X)
max_data = max(X)
X=np.asarray(X).reshape((size,1))
boundary = np.linspace(min_data-0.01, max_data, nbin)
ax[0, 0].hist(X[:,0], bins=boundary, fc='#AAAAFF', **density_param)
ax[0, 0].text(-.07, 17, "Amygdala Histogram")

# KDE amygdala
plt.subplot(2, 2, 3)
pd.Series(X.T[0]).plot.kde(0.25)

# histogram acc
X=acc
min_data = min(X)
max_data = max(X)
X=np.asarray(X).reshape((size,1))
boundary = np.linspace(min_data-0.01, max_data, nbin)
ax[0, 1].hist(X[:,0], bins=boundary, fc='#AAAAFF', **density_param)
ax[0, 1].text(-.025, 32, "ACC Histogram")

# KDE acc
plt.subplot(2, 2, 4)
pd.Series(X.T[0]).plot.kde(0.3)
# for 2 dimensional data
pdata=data[1:,:2].astype(float)
min_data = pdata.min(0)
max_data = pdata.max(0)


nbin = 30        # you can change the number of bins in each dimension
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
hist, xedges, yedges = np.histogram2d(pdata[:,0], pdata[:,1], bins=nbin)
xpos, ypos = np.meshgrid(xedges[:-1]+xedges[1:], yedges[:-1]+yedges[1:])
xpos = xpos.flatten()/2.
ypos = ypos.flatten()/2.
zpos = np.zeros_like (xpos)
dx = xedges [1] - xedges [0]
dy = yedges [1] - yedges [0]
dz = hist.flatten()
ax.bar3d(xpos, ypos, zpos, dx, dy, dz )
#kernel density estimator
# create an evaluation grid
gridno = 40
inc1 = (max_data[0]-min_data[0])/gridno
inc2 = (max_data[1]-min_data[1])/gridno
gridx, gridy = np.meshgrid( np.arange(min_data[0], max_data[0]+inc1,inc1), np.arange(min_data[1], max_data[1]+inc2,inc2) )
gridall = [gridx.flatten(order = 'F'), gridy.flatten(order = 'F')]
gridall = (np.asarray(gridall)).T
gridallno, nn= gridall.shape
norm_pdata = (np.power(pdata, 2)).sum(axis=1)
norm_gridall = (np.power(gridall, 2)).sum(axis=1)
cross = np.dot(pdata,gridall.T)
#dist2 = np.matlib.repmat(norm_pdata, 1, gridallno)
dist2 = np.repeat(norm_pdata, repeats =gridallno).reshape((len(norm_pdata), gridallno))+np.tile(norm_gridall, size).reshape((len(norm_pdata), gridallno)) - 2* cross


bandwidth = 0.0002
kernelvalue = np.exp(-dist2/bandwidth)
mkde = sum(kernelvalue) / size
mkde = ((mkde.T).reshape(gridx.shape[1], gridx.shape[0])).T
fig = plt.figure()
ax=fig.add_subplot(111, projection='3d')
ax.plot_surface(gridx, gridy, mkde)
plt.show()


print(gridx.shape)

#################################
data=pd.read_csv(path,header=0)
data[['amygdala', 'acc']] = data[['amygdala', 'acc']].astype(float)
data['orientation'] = data['orientation'].astype(int)
Q4_mean=[]
Q4=[]
cols = data.columns 
for j in range(2):
    for i in range(2,6):
        temp=data[data.iloc[:, 2] == i] .iloc[:, j]
        Q4.append(temp)
        Q4_mean.append(temp.mean())
        plt.subplot(2, 4, i-1+j*4)
        pd.Series(temp).plot.kde(0.3)
        plt.title(cols[j]+" "+str(i))
 ###########################################

for temp in Q4:
    pd.Series(temp).plot.kde(0.3)

table=pd.DataFrame([Q4_mean[:4],Q4_mean[4:]])
table.columns=['C=2', 'C=3', 'C=4', 'C=5']
table.insert(0, 'Conditions',['amygdala','acc'])
table.set_index('Conditions')
display (table)


###############################

Qe=[]
for i in range(2,6):
    temp=data[data.iloc[:, 2] == i]
    Qe.append(temp)
Qe[0]
#Q4_mean[4:]
data3=data.to_numpy()[:,:2]
#['C=2', 'C=3', 'C=4', 'C=5']
plt.subplot(2, 2, 1)
sb.kdeplot(Qe[0]['amygdala'],Qe[0]['acc'],shade=True, bw=0.015)
plt.subplot(2, 2, 2)
sb.kdeplot(Qe[1]['amygdala'],Qe[1]['acc'],shade=True, bw=0.015)
plt.subplot(2, 2, 3)
sb.kdeplot(Qe[2]['amygdala'],Qe[2]['acc'],shade=True, bw=0.015)
plt.subplot(2, 2, 4)
sb.kdeplot(Qe[3]['amygdala'],Qe[3]['acc'],shade=True, bw=0.015)
sb.pairplot(data, hue="orientation")

###############################

from scipy.stats import ttest_ind
ttest_ind(data['amygdala'],data['acc'])

################################################################################
#Q2
import numpy as np
import numpy.matlib
import pandas as pd
import numpy as ppool
from scipy.stats import multivariate_normal as mvn
import scipy.io
import matplotlib.pyplot as plt
from sklearn import preprocessing
import random

def Gaussian(data,mu,sigma):
    temp=[]
    for i in data:
        temp.append(np.exp(-0.5*(i-mu).T@np.linalg.inv(sigma)@(i-mu))/((np.linalg.det(sigma))**0.5))
    return np.asarray(temp)


mat = scipy.io.loadmat('data/data.mat')
ndata=mat['data'].T
label = scipy.io.loadmat('data/label.mat')
y=label['trueLabel'].T


m, n = ndata.shape
C = np.matmul(ndata.T, ndata)/m

# pca the data
d = 4  # reduced dimension
V,_,_ = np.linalg.svd(C)
V = V[:, :d]

# project the data to the top 2 principal directions
pdata = np.dot(ndata,V)
#plt.show()

# EM-GMM for wine data
# number of mixtures
K = 2

# random seed
seed = 4

# initialize prior
np.random.seed(seed)
pi = np.random.random(K)
pi = pi/np.sum(pi)

# initial mean and covariance
np.random.seed(seed)
mu = np.random.randn(K,d)
mu_old = mu.copy()

sigma = []
for ii in range(K):
    # to ensure the covariance psd
    np.random.seed(seed)
    dummy = np.random.randn(d, d)
    sigma.append(dummy@dummy.T+ppool.identity(d))
    
# initialize the posterior
tau = np.full((m, K), fill_value=0.)

# # parameter for countour plot
# xrange = np.arange(-5, -5, 0.1)
# yrange = np.arange(-5, -5, 0.1)

# ####
maxIter= 2
tol = 1e-3

plt.ion()
mu_list=[]

for ii in range(100):
    logtau=[]
    # E-step    
    for kk in range(K):
        tau[:, kk] = pi[kk] * Gaussian(pdata, mu[kk], sigma[kk])
    # normalize tau
    sum_tau = np.sum(tau, axis=1)
    mu_list.append(np.mean(np.log(sum_tau)))
    sum_tau.shape = (m,1)    
    tau = np.divide(tau, np.tile(sum_tau, (1, K)))
    
    
    # M-step
    for kk in range(K):
        # update prior
        pi[kk] = np.sum(tau[:, kk])/m
        
        # update component mean
        mu[kk] = pdata.T @ tau[:,kk] / np.sum(tau[:,kk], axis = 0)
        
        # update cov matrix
        dummy = pdata - np.tile(mu[kk], (m,1)) # X-mu
        sigma[kk] = dummy.T @ np.diag(tau[:,kk]) @ dummy / np.sum(tau[:,kk], axis = 0)
  
    if np.linalg.norm(mu-mu_old) < tol:
        break
    mu_old = mu.copy()
    if ii==99:
        print('max iteration reached')
        break
y_new=[]
for gg in range(len(tau)):
    if tau[gg,0]>tau[gg,1]:
        y_new.append(2)
    else:
        y_new.append(2)


plt.plot(mu_list)

#########################
import seaborn as sns
covMatrix0 = np.cov(sigma[0],bias=True)
covMatrix1 = np.cov(sigma[1],bias=True)
sns.heatmap(covMatrix0)
sns.heatmap(covMatrix1)


###############################
print(np.exp(-0.5*(i-mu[kk]).T@sigma[kk].T@(i-mu[kk]))/((np.linalg.det(sigma[kk])**0.5)))
def Gaussian(data,mu,sigma):
    temp=[]
    for i in data:
        temp.append(np.exp(-0.5*(i-mu).T@sigma.T@(i-mu))/(np.linalg.det(sigma))**0.5)
    return np.asarray(temp)
Gaussian(pdata, mu[kk], sigma[kk])
####################################

E step: 
    Find the posterior distribution:
        N(x|mu,sigma) = exp(-0.5*(x-mu).T dot inv(sigma) dot (x-mu))/((det(sigma))**0.5)
        Prior: p(z)=pi(z)
        Likelihood: p(x|z)=N(x|mu(z),sigma(z))
        Posterior: p(z|x)=(pi(z)*N(x|mu(z),sigma(z)))/(sum((pi(z)*N(x|mu(z),sigma(z)))))
        tau(i,k)=p(z|x)=(pi(k)*N(x(i)|mu(k),sigma(k)))/(sum((pi(k)*N(x|mu(k),sigma(k)))))   
    Compute the expectation:
        f(theta)=sum from i =1 to m (E(p(z(i)|x(i),theta(t)))*[log(p(x(i),z(i)|theta))]) 
                =sum from i =1 to m (E(p(z(i)|x(i),theta(t)))*[log(pt(z,i)*N(x(i)|mu(z,k),sigma(z,k)))]) 
                =sum from i =1 to m, i=1 to k (tau(k,i)(log(pi(k))-0.5(x(i)-mu(k)).T dot inv(sigma(k)) dot (x(i)-mu(k))
                -0.5*(det(sigma))-n/2log(2*pi)))

M step:
    Maximze f(theta):
        f(theta)=sum from i =1 to m, i=1 to k (tau(k,i)(log(pi(k))-0.5(x(i)-mu(k)).T dot inv(sigma(k)) dot (x(i)-mu(k))
                -0.5*(det(sigma))-n/2log(2*pi)))
        sum(pi(k))=1
        Form Lagrangian:
            L=sum from i =1 to m, i=1 to k (tau(k,i)(log(pi(k))-0.5(x(i)-mu(k)).T dot inv(sigma(k)) dot (x(i)-mu(k))
                -0.5*(det(sigma))-n/2log(2*pi))) + lamuda*(1-sum(pi(k)))
         Find the Maxmum, derivative =0:
             dL/d(pi(k))=sum i=1 to m (tau(k,i)/pi(k)-lamua)=0
             pi(k)=1/lamda*sum i=1 to m (tau(k,i))   
         sum(pi(k))=1: => lamda =m =>pi(k)=1/m*sum i=1 to m (tau(k,i))
         new mu = (sum i=1 to m (tau(k,i) dot x(i)))/(sum i=1 to m (tau(k,i)))
         new sigma = (sum i =1 tom (tau(k,i))*(x(i)-mu(k)))/(sum i=1 to m (tau(k,i)))
         
         
Then use new mu and sigma repeat E_M until we have the change of mu under a threshhold, we stop.
report mu, pi, and sigma as result.

##############################
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from PIL import Image
from numpy import asarray
import math
import numpy.matlib
from scipy.stats import multivariate_normal as mvn
import scipy.io
from sklearn import preprocessing


df2 = pd.read_csv('face_data.csv')
subject=[i for i in range(df2['target'].nunique())]
print(len(subject))
df2.head()
#####################
data = df2.drop('target', axis=1)
fig, axes = plt.subplots(10, 5, figsize=(6, 6))
for i, ax in enumerate(axes.flat):
    ax.imshow(np.array(data)[i].reshape(64, 64), cmap='gray')
plt.show()
#####################
data_y = df2['target']
X=np.array(data)
y= df2['target']

####################

X_train, X_test, y_train, y_test = train_test_split(X, y)
pca = PCA().fit(X_train)
plt.figure(figsize=(18, 7))
plt.plot(pca.explained_variance_ratio_.cumsum(), lw=3)
####################
plt.imshow(pca.mean_.reshape(64,64),
           cmap=plt.cm.bone)
fig = plt.figure(figsize=(16, 6))
for i in range(30):
    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])
    ax.imshow(pca.components_[i].reshape(64,64),
              cmap=plt.cm.bone)
####################
test=np.where(pca.explained_variance_ratio_.cumsum() > 0.95)
pca = PCA(n_components=105).fit(X_train)
#####################
X_train_pca = pca.transform(X_train)
X_train_pca.shape

#####################
def NB(X,y):
    gnb = GaussianNB(var_smoothing=1e-3)
    return gnb.fit(X, y)
nb=NB(X_train,y_train)
nb.predict(X_test)
#####################
def KNN(X,y):
    neigh = KNeighborsClassifier(n_neighbors=3)
    return neigh.fit(X_train,y_train)
knn=KNN(X_train,y_train)
knn.predict(X_test)

#############################
def em_alg(X,y):
    m, n = X.shape
    C = np.matmul(X.T, X)/m

    # pca the data
    d = 4  # reduced dimension
    V,_,_ = np.linalg.svd(C)
    V = V[:, :d]

    # project the data to the top 2 principal directions
    pdata = np.dot(X,V)

    plt.scatter(pdata[np.where(y == 2),0],pdata[np.where(y == 2),1])
    plt.scatter(pdata[np.where(y == 6),0],pdata[np.where(y == 6),1])
    #plt.show()
    K = 40
    # random seed
    seed = 4

    # initialize prior
    # np.random.seed(seed)
    pi = np.random.random(K)
    pi = pi/np.sum(pi)

    # initial mean and covariance
    # np.random.seed(seed)
    mu = np.random.randn(K,d)
    mu_old = mu.copy()

    sigma = []
    for ii in range(K):
        # to ensure the covariance psd
        # np.random.seed(seed)
        dummy = np.random.randn(d, d)
        sigma.append(dummy@dummy.T+np.identity(d))

    tau = np.full((m, K), fill_value=0.)
    maxIter= 2
    tol = 1e-3

    plt.ion()
    mu_list=[]

    for ii in range(10):

        # E-step    
        for kk in range(K):
            tau[:, kk] = pi[kk] * mvn.pdf(pdata, mu[kk], sigma[kk])
        

        # normalize tau
        sum_tau = np.sum(tau, axis=1)
        sum_tau.shape = (m,1)    
        if ii>0:
            mu_list.append(np.mean(np.log(sum_tau)))
        tau = np.divide(tau, np.tile(sum_tau, (1, K)))

       # M-step
        for kk in range(K):
            # update prior
            pi[kk] = np.sum(tau[:, kk])/m

            # update component mean
            mu[kk] = pdata.T @ tau[:,kk] / np.sum(tau[:,kk], axis = 0)

            # update cov matrix
            dummy = pdata - np.tile(mu[kk], (m,1)) # X-mu
            sigma[kk] = dummy.T @ np.diag(tau[:,kk]) @ dummy / np.sum(tau[:,kk], axis = 0)


        print('-----iteration---',ii)    

        if np.linalg.norm(mu-mu_old) < tol:
            print('training coverged')
            break
        mu_old = mu.copy()
        if ii==10:
            print('max iteration reached')

            break
        return mu_list, mu, sigma, V

############################
plt.imshow((V@(mu[0])).T.reshape((64,64)), cmap=plt.cm.gray)
